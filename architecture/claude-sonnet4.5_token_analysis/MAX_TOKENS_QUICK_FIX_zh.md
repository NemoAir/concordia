# ğŸš¨ Concordia Max Tokens é—®é¢˜å¿«é€Ÿä¿®å¤æŒ‡å—

> **ç´§æ€¥ç¨‹åº¦ï¼š** é«˜
> **å½±å“èŒƒå›´ï¼š** æ‰€æœ‰ä½¿ç”¨Geminiæ¨¡å‹çš„éƒ¨ç½²
> **é¢„è®¡ä¿®å¤æ—¶é—´ï¼š** 30åˆ†é’Ÿï¼ˆç´§æ€¥ä¿®å¤ï¼‰â†’ 2å°æ—¶ï¼ˆå®Œæ•´ä¿®å¤ï¼‰

---

## ğŸ“Œ é—®é¢˜é€Ÿè§ˆ

### ç—‡çŠ¶
- ç¨‹åºè¿è¡Œæ—¶æŠ›å‡º `IndexError: list index out of range`
- Geminiæ¨¡å‹è¿”å›ç©ºå“åº”ï¼Œå¯¼è‡´ `random.choice([])` å¤±è´¥
- ä¸–ç•Œæ¨¡æ‹Ÿè´¨é‡ä¸‹é™ï¼Œå‡ºç°ä¸è¿è´¯çš„å¯¹è¯æˆ–äº‹ä»¶

### æ ¹æœ¬åŸå› 
1. **Geminiç‰¹æ®Šé™åˆ¶ï¼š** max_tokens < 256 æ—¶å¯èƒ½è¿”å›ç©ºå“åº”
2. **å¼‚å¸¸å¤„ç†ä¸å®Œæ•´ï¼š** åªæ•è· `ValueError`ï¼Œæœªæ•è· `IndexError`
3. **ç©ºå€¼æœªæ£€æŸ¥ï¼š** ç›´æ¥è®¿é—®æ•°ç»„ç¬¬ä¸€ä¸ªå…ƒç´ ï¼Œæœªæ£€æŸ¥æ•°ç»„æ˜¯å¦ä¸ºç©º

---

## ğŸ”§ ç«‹å³ä¿®å¤ï¼ˆ5åˆ†é’Ÿï¼‰

### æ–¹æ¡ˆ1ï¼šå…¨å±€é…ç½®è°ƒæ•´ï¼ˆä¸ä¿®æ”¹æºç ï¼‰

åœ¨ä½ çš„å¯åŠ¨è„šæœ¬ **æœ€å‰é¢** æ·»åŠ ï¼š

```python
# fix_max_tokens.py
import concordia.document.interactive_document as interactive_doc

# æé«˜å…¨å±€é»˜è®¤å€¼
interactive_doc.DEFAULT_MAX_CHARACTERS = 2000  # åŸå€¼: 200
interactive_doc.DEFAULT_MAX_TOKENS = 500       # åŸå€¼: 50

print("âœ… Max tokens defaults increased to prevent Gemini errors")
```

**ç„¶ååœ¨ä½ çš„ä¸»ç¨‹åºä¸­ï¼š**

```python
# åœ¨import concordiaä¹‹åï¼Œåˆ›å»ºä»»ä½•å¯¹è±¡ä¹‹å‰
import fix_max_tokens

# ç°åœ¨ç»§ç»­ä½ çš„ä»£ç 
from concordia import ...
```

### æ–¹æ¡ˆ2ï¼šMonkey Patch ç´§æ€¥ä¿®å¤ï¼ˆä¸ä¿®æ”¹æºç ï¼‰

```python
# emergency_patch.py
import concordia.document.interactive_document as interactive_doc
import random

# ä¿å­˜åŸæ–¹æ³•
_original_open_question_diversified = interactive_doc.InteractiveDocument.open_question_diversified

def safe_open_question_diversified(self, question, **kwargs):
    """å®‰å…¨åŒ…è£…ç‰ˆæœ¬ï¼Œé˜²æ­¢IndexError"""
    # ç¡®ä¿max_tokensä¸ä½äºGeminiæœ€å°å€¼
    max_tokens = kwargs.get('max_tokens', 50)
    num_samples = kwargs.get('num_samples', 10)
    kwargs['max_tokens'] = max(max_tokens, 256 // num_samples)

    try:
        return _original_open_question_diversified(self, question, **kwargs)
    except (IndexError, ValueError) as e:
        print(f"âš ï¸ open_question_diversified failed: {e}")
        print("   Falling back to simple open_question")
        # é™çº§åˆ°ç®€å•ç‰ˆæœ¬
        return self.open_question(
            question=question,
            max_tokens=max(1000, kwargs.get('max_tokens', 500) * 3),
            temperature=kwargs.get('temperature', 1.0),
        )

# æ›¿æ¢åŸæ–¹æ³•
interactive_doc.InteractiveDocument.open_question_diversified = safe_open_question_diversified

print("âœ… Emergency patch applied for open_question_diversified")
```

**ä½¿ç”¨ï¼š**

```python
import emergency_patch
from concordia import ...
```

---

## ğŸ› ï¸ å®Œæ•´ä¿®å¤ï¼ˆ30åˆ†é’Ÿï¼‰

### ä¿®å¤1: google_aistudio_model.py

**æ–‡ä»¶ï¼š** `concordia/language_model/google_aistudio_model.py`
**è¡Œå·ï¼š** 177-183

**ä¿®æ”¹å‰ï¼š**
```python
try:
    response = sample.candidates[0].content.parts[0].text
except ValueError as e:  # âŒ åªæ•è·ValueError
    print('An error occurred: ', e)
    print(f'prompt: {prompt}')
    print(f'sample: {sample}')
    response = ''
```

**ä¿®æ”¹åï¼š**
```python
try:
    # 1. é¦–å…ˆæ£€æŸ¥candidatesæ˜¯å¦ä¸ºç©º
    if not sample.candidates:
        raise IndexError('No candidates returned by the model')

    # 2. å°è¯•è·å–å“åº”
    response = sample.candidates[0].content.parts[0].text

    # 3. æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
    if not response or not response.strip():
        raise ValueError('Empty response from model')

except (ValueError, IndexError, AttributeError) as e:  # âœ… å®Œæ•´å¼‚å¸¸æ•è·
    print(f'âŒ LLM Error: {type(e).__name__}: {e}')
    print(f'Prompt (first 200 chars): {prompt[:200]}...')

    # æ£€æŸ¥å¤±è´¥åŸå› 
    if sample.candidates and hasattr(sample.candidates[0], 'finish_reason'):
        finish_reason = sample.candidates[0].finish_reason
        print(f'Finish reason: {finish_reason}')

        if 'SAFETY' in str(finish_reason):
            response = '[BLOCKED_BY_SAFETY]'
        elif 'MAX_TOKENS' in str(finish_reason):
            response = '[TRUNCATED_BY_MAX_TOKENS]'
        else:
            response = '[GENERATION_FAILED]'
    else:
        response = '[NO_CANDIDATES]'
```

**åŒæ ·çš„ä¿®æ”¹é€‚ç”¨äºï¼š** `concordia/language_model/cloud_vertex_model.py` (è¡Œ134-140)

### ä¿®å¤2: interactive_document.py

**æ–‡ä»¶ï¼š** `concordia/document/interactive_document.py`
**è¡Œå·ï¼š** 289-294

**ä¿®æ”¹å‰ï¼š**
```python
if len(candidates) < 2:
    raise Warning(  # âŒ Warningä¸ä¼šä¸­æ–­æ‰§è¡Œ
        f'LLM generated only {len(candidates)} initial answers.'
    )
candidates = [re.sub(r'^\d+\.\s*', '', line) for line in candidates]
response = random.choice(candidates)  # âŒ å¯èƒ½IndexError
```

**ä¿®æ”¹åï¼š**
```python
if len(candidates) < 2:
    self.debug(f'LLM generated only {len(candidates)} initial answers.')

    # âœ… æä¾›é™çº§æ–¹æ¡ˆ
    if len(candidates) == 0:
        # å®Œå…¨å¤±è´¥ï¼Œé™çº§åˆ°ç®€å•é—®ç­”
        return self.open_question(
            question=question,
            max_tokens=max_tokens * 5,  # ç»™æ›´å¤štoken
            temperature=min(temperature + 0.3, 1.5),
            question_label=question_label,
            answer_label=answer_label,
        )
    elif len(candidates) == 1:
        # åªæœ‰ä¸€ä¸ªå€™é€‰ï¼Œç›´æ¥ä½¿ç”¨
        response = candidates[0]
    else:
        # ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼Œä½†ä»¥é˜²ä¸‡ä¸€
        raise ValueError(
            f'Unexpected candidates count: {len(candidates)}'
        )
else:
    # âœ… æ­£å¸¸è·¯å¾„
    candidates = [re.sub(r'^\d+\.\s*', '', line) for line in candidates]
    response = random.choice(candidates)
```

### ä¿®å¤3: ç¡®ä¿max_tokensæœ€å°å€¼

**åœ¨è°ƒç”¨å‰æ£€æŸ¥ï¼š**

```python
# åœ¨æ‰€æœ‰è°ƒç”¨LLMçš„åœ°æ–¹æ·»åŠ 
def ensure_min_max_tokens(max_tokens: int, min_value: int = 256) -> int:
    """ç¡®ä¿max_tokensä¸ä½äºæœ€å°å€¼ï¼ˆGeminiè¦æ±‚ï¼‰"""
    return max(max_tokens, min_value)

# ä½¿ç”¨ç¤ºä¾‹
response = model.sample_text(
    prompt=prompt,
    max_tokens=ensure_min_max_tokens(original_max_tokens),  # âœ…
    ...
)
```

---

## ğŸ“Š é…ç½®å»ºè®®

### æ¨èçš„max_tokenså€¼ï¼ˆåŸºäºæºç åˆ†æï¼‰

| ç»„ä»¶ | åŸå§‹å€¼ | æ¨èå€¼ï¼ˆæœ€å°ï¼‰ | æ¨èå€¼ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰ |
|------|--------|----------------|---------------------|
| DEFAULT_MAX_TOKENS | 50 | 256 | 512 |
| question_of_recent_memories | 1000 | 1500 | 2500 |
| all_similar_memories (æ‘˜è¦) | 750 | 1000 | 1500 |
| concat_act_component | 2200 | 2200 | 3000 |
| open_question_diversified | 50Ã—N | max(256, 100Ã—N) | max(512, 150Ã—N) |
| thought_chains (å„æ­¥éª¤) | 1200-3500 | 1500-4000 | 2000-5000 |

### å®æ–½æ–¹å¼

**é€‰é¡¹Aï¼šä¿®æ”¹æºç ä¸­çš„é»˜è®¤å€¼**

```python
# concordia/document/interactive_document.py
DEFAULT_MAX_CHARACTERS = 2000  # åŸ: 200
DEFAULT_MAX_TOKENS = 500       # åŸ: 50
```

**é€‰é¡¹Bï¼šè¿è¡Œæ—¶è¦†ç›–ï¼ˆæ¨èç”¨äºæµ‹è¯•ï¼‰**

```python
# åœ¨åˆ›å»ºç»„ä»¶æ—¶æ˜¾å¼æŒ‡å®š
reflection = question_of_recent_memories.QuestionOfRecentMemories(
    model=model,
    question="...",
    max_tokens=2500,  # âœ… æ˜¾å¼æŒ‡å®š
    ...
)
```

---

## âœ… éªŒè¯ä¿®å¤æ•ˆæœ

### æµ‹è¯•æ¸…å•

- [ ] **åŸºç¡€æµ‹è¯•ï¼š** è¿è¡Œä¸€ä¸ªç®€å•çš„agent actå¾ªç¯ï¼Œç¡®ä¿ä¸æŠ›å‡ºå¼‚å¸¸
- [ ] **å‹åŠ›æµ‹è¯•ï¼š** è¿è¡Œ100æ­¥æ¸¸æˆå¾ªç¯ï¼Œç›‘æ§å¼‚å¸¸æ—¥å¿—
- [ ] **è¾¹ç•Œæµ‹è¯•ï¼š** æ•…æ„æä¾›å¤æ‚promptï¼Œè§‚å¯Ÿæ˜¯å¦æ­£ç¡®å¤„ç†æˆªæ–­
- [ ] **Geminiç‰¹å®šï¼š** æ£€æŸ¥æ—¥å¿—ä¸­æ˜¯å¦è¿˜æœ‰ `[NO_CANDIDATES]` æˆ– `[BLOCKED_BY_SAFETY]`

### ç›‘æ§æŒ‡æ ‡

```python
# æ·»åŠ ç®€å•çš„è®¡æ•°å™¨
class LLMCallMonitor:
    def __init__(self):
        self.total_calls = 0
        self.empty_responses = 0
        self.truncated_responses = 0
        self.exceptions = 0

    def record_call(self, response: str, exception: bool = False):
        self.total_calls += 1
        if exception:
            self.exceptions += 1
        elif not response or response.startswith('['):
            self.empty_responses += 1
        elif response.endswith('...'):  # ç®€åŒ–çš„æˆªæ–­æ£€æµ‹
            self.truncated_responses += 1

    def report(self):
        print(f"ğŸ“Š LLM Call Statistics:")
        print(f"   Total calls: {self.total_calls}")
        print(f"   Empty responses: {self.empty_responses} ({self.empty_responses/self.total_calls*100:.1f}%)")
        print(f"   Truncated: {self.truncated_responses} ({self.truncated_responses/self.total_calls*100:.1f}%)")
        print(f"   Exceptions: {self.exceptions} ({self.exceptions/self.total_calls*100:.1f}%)")

# ä½¿ç”¨
monitor = LLMCallMonitor()
# ... åœ¨LLMè°ƒç”¨å ...
monitor.record_call(response)
# ... åœ¨ç¨‹åºç»“æŸæ—¶ ...
monitor.report()
```

**å¥åº·æŒ‡æ ‡ï¼š**
- ç©ºå“åº”ç‡ < 1%
- æˆªæ–­ç‡ < 15%
- å¼‚å¸¸ç‡ < 0.1%

---

## ğŸ¯ æ•ˆæœå¯¹æ¯”

### ä¿®å¤å‰
```
Step 1/100: âœ“
Step 2/100: âœ“
Step 3/100: âŒ IndexError: list index out of range
  File "interactive_document.py", line 294, in open_question_diversified
    response = random.choice(candidates)
```

### ä¿®å¤å
```
Step 1/100: âœ“
Step 2/100: âœ“
Step 3/100: âš ï¸ open_question_diversified got 0 candidates, falling back
Step 3/100: âœ“ (fallback successful)
Step 4/100: âœ“
...
Step 100/100: âœ“

ğŸ“Š LLM Call Statistics:
   Total calls: 437
   Empty responses: 3 (0.7%)
   Truncated: 28 (6.4%)
   Exceptions: 0 (0.0%)
```

---

## ğŸ“š è¿›ä¸€æ­¥é˜…è¯»

è¯¦ç»†çš„åˆ†æå’Œé•¿æœŸè§£å†³æ–¹æ¡ˆï¼Œè¯·å‚è€ƒï¼š
- **å®Œæ•´åˆ†ææ–‡æ¡£ï¼š** `architecture/MAX_TOKENS_PROBLEM_ANALYSIS_zh.html`
- **Tokenåˆ†æï¼š** `architecture/concordia_token_analysis_zh.html`
- **å¢é•¿åˆ†æï¼š** `architecture/concordia_token_growth_analysis_zh.html`

---

## ğŸ†˜ å¸¸è§é—®é¢˜

### Q1: ä¿®å¤åæˆæœ¬ä¼šå¢åŠ å¤šå°‘ï¼Ÿ

**A:** ä¿å®ˆé…ç½®ï¼ˆæ¨èå€¼æœ€å°ï¼‰å¢åŠ çº¦20-30%ï¼Œè´¨é‡ä¼˜å…ˆé…ç½®å¢åŠ çº¦50-80%ã€‚ä½†è€ƒè™‘åˆ°é¿å…äº†å¤±è´¥é‡è¯•å’Œé™çº§ï¼Œå®é™…å¢åŠ å¯èƒ½æ›´å°‘ã€‚

### Q2: æ˜¯å¦éœ€è¦åŒæ—¶ä¿®æ”¹æ‰€æœ‰æ–‡ä»¶ï¼Ÿ

**A:**
- **ç´§æ€¥æƒ…å†µï¼š** åªéœ€è¦åº”ç”¨"æ–¹æ¡ˆ1"æˆ–"æ–¹æ¡ˆ2"çš„monkey patch
- **æ­£å¼ä¿®å¤ï¼š** å»ºè®®è‡³å°‘ä¿®æ”¹ `google_aistudio_model.py` å’Œ `interactive_document.py`
- **é•¿æœŸä¼˜åŒ–ï¼š** æŒ‰ç…§å®Œæ•´åˆ†ææ–‡æ¡£è¿›è¡Œæ¶æ„å‡çº§

### Q3: æ˜¯å¦å½±å“å…¶ä»–æ¨¡å‹ï¼ˆå¦‚GPT-4ï¼‰ï¼Ÿ

**A:**
- **å¼‚å¸¸å¤„ç†ä¿®å¤ï¼š** å¯¹æ‰€æœ‰æ¨¡å‹æœ‰ç›Šï¼Œæé«˜ç¨³å®šæ€§
- **max_tokensè°ƒæ•´ï¼š** GPT-4æ²¡æœ‰256çš„æœ€å°é™åˆ¶ï¼Œä½†æ›´é«˜çš„å€¼å¯ä»¥æ”¹å–„è¾“å‡ºè´¨é‡
- **å»ºè®®ï¼š** ä½¿ç”¨æ¡ä»¶åˆ¤æ–­ï¼ŒGeminiä½¿ç”¨æ›´é«˜å€¼ï¼Œå…¶ä»–æ¨¡å‹ä¿æŒåŸå€¼

### Q4: å¦‚ä½•åˆ¤æ–­æ˜¯å¦å·²ç»éƒ¨ç½²äº†ä¿®å¤ï¼Ÿ

**A:** è¿è¡Œä»¥ä¸‹æ£€æŸ¥è„šæœ¬ï¼š

```python
# check_fix.py
import concordia.document.interactive_document as doc
import concordia.language_model.google_aistudio_model as gemini_model
import inspect

# æ£€æŸ¥DEFAULT_MAX_TOKENS
if doc.DEFAULT_MAX_TOKENS >= 256:
    print("âœ… DEFAULT_MAX_TOKENSå·²ä¿®å¤:", doc.DEFAULT_MAX_TOKENS)
else:
    print("âŒ DEFAULT_MAX_TOKENSä»éœ€ä¿®å¤:", doc.DEFAULT_MAX_TOKENS)

# æ£€æŸ¥å¼‚å¸¸æ•è·
source = inspect.getsource(gemini_model.GoogleAIStudioLanguageModel.sample_text)
if 'IndexError' in source and 'AttributeError' in source:
    print("âœ… å¼‚å¸¸æ•è·å·²å¢å¼º")
else:
    print("âŒ å¼‚å¸¸æ•è·ä»éœ€ä¿®å¤")

# æ£€æŸ¥open_question_diversified
source = inspect.getsource(doc.InteractiveDocument.open_question_diversified)
if 'len(candidates) == 0' in source or 'if not candidates' in source:
    print("âœ… open_question_diversifiedå·²ä¿®å¤")
else:
    print("âŒ open_question_diversifiedä»éœ€ä¿®å¤")
```

---

**æœ€åæ›´æ–°ï¼š** 2025-01-10
**é€‚ç”¨ç‰ˆæœ¬ï¼š** Concordia ä¸»åˆ†æ”¯

æœ‰é—®é¢˜ï¼ŸæŸ¥çœ‹ [å®Œæ•´åˆ†ææ–‡æ¡£](./MAX_TOKENS_PROBLEM_ANALYSIS_zh.html) æˆ–æäº¤Issueã€‚
