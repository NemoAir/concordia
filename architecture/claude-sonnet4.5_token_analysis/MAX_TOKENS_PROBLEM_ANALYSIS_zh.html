<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Concordia Max Tokens é—®é¢˜åˆ†æä¸è§£å†³æ–¹æ¡ˆ</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, Arial, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .container {
            background-color: white;
            padding: 30px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        h1 {
            color: #d32f2f;
            border-bottom: 3px solid #d32f2f;
            padding-bottom: 10px;
        }
        h2 {
            color: #1976d2;
            margin-top: 30px;
            border-left: 4px solid #1976d2;
            padding-left: 10px;
        }
        h3 {
            color: #388e3c;
            margin-top: 20px;
        }
        .problem-box {
            background-color: #ffebee;
            border-left: 4px solid #d32f2f;
            padding: 15px;
            margin: 15px 0;
        }
        .solution-box {
            background-color: #e8f5e9;
            border-left: 4px solid #388e3c;
            padding: 15px;
            margin: 15px 0;
        }
        .warning-box {
            background-color: #fff3e0;
            border-left: 4px solid #f57c00;
            padding: 15px;
            margin: 15px 0;
        }
        .info-box {
            background-color: #e3f2fd;
            border-left: 4px solid #1976d2;
            padding: 15px;
            margin: 15px 0;
        }
        code {
            background-color: #f5f5f5;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
            font-size: 0.9em;
        }
        pre {
            background-color: #263238;
            color: #aed581;
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            line-height: 1.4;
        }
        pre code {
            background-color: transparent;
            color: #aed581;
            padding: 0;
        }
        .code-ref {
            color: #1976d2;
            font-family: monospace;
            font-size: 0.9em;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 15px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        th {
            background-color: #1976d2;
            color: white;
        }
        tr:nth-child(even) {
            background-color: #f5f5f5;
        }
        .severity-high {
            color: #d32f2f;
            font-weight: bold;
        }
        .severity-medium {
            color: #f57c00;
            font-weight: bold;
        }
        .severity-low {
            color: #388e3c;
        }
        ul.checklist {
            list-style-type: none;
            padding-left: 0;
        }
        ul.checklist li:before {
            content: "âœ“ ";
            color: #388e3c;
            font-weight: bold;
            margin-right: 8px;
        }
        .flow-diagram {
            background-color: #fafafa;
            border: 2px solid #e0e0e0;
            padding: 20px;
            margin: 20px 0;
            border-radius: 8px;
            font-family: monospace;
            white-space: pre;
            overflow-x: auto;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>ğŸš¨ Concordia Max Tokens é—®é¢˜æ·±åº¦åˆ†æä¸è§£å†³æ–¹æ¡ˆ</h1>

        <div class="info-box">
            <strong>æ–‡æ¡£æ¦‚è¿°</strong><br>
            æœ¬æ–‡æ¡£åˆ†æäº†Concordiaæ¡†æ¶åœ¨å®é™…è¿è¡Œä¸­é‡åˆ°çš„max_tokensé™åˆ¶é—®é¢˜ï¼ŒåŒ…æ‹¬ï¼š
            <ul>
                <li>Geminiæ¨¡å‹è¿”å›ç©ºå“åº”å¯¼è‡´çš„ç¨‹åºå¼‚å¸¸</li>
                <li>max_tokensè®¾è®¡é™åˆ¶ä¸LLMè‡ªç„¶å‘æŒ¥çš„çŸ›ç›¾</li>
                <li>å½“å‰ä»£ç ä¸­çš„å¼‚å¸¸å¤„ç†ç¼ºé™·</li>
                <li>å®Œæ•´çš„å¤šå±‚æ¬¡è§£å†³æ–¹æ¡ˆ</li>
            </ul>
            <strong>åŸºäºæºç ç‰ˆæœ¬ï¼š</strong> å½“å‰Concordiaä¸»åˆ†æ”¯<br>
            <strong>åˆ†ææ—¥æœŸï¼š</strong> 2025-01-10
        </div>

        <h2>ğŸ“‹ ç›®å½•</h2>
        <ol>
            <li><a href="#problem-root">é—®é¢˜æ ¹æºåˆ†æ</a></li>
            <li><a href="#failure-scenarios">å…¸å‹å¤±è´¥åœºæ™¯</a></li>
            <li><a href="#code-defects">ä»£ç ç¼ºé™·å®šä½</a></li>
            <li><a href="#solutions">å¤šå±‚æ¬¡è§£å†³æ–¹æ¡ˆ</a></li>
            <li><a href="#implementation">ä»£ç å®ç°æ–¹æ¡ˆ</a></li>
            <li><a href="#best-practices">æœ€ä½³å®è·µä¸å»ºè®®</a></li>
        </ol>

        <h2 id="problem-root">1ï¸âƒ£ é—®é¢˜æ ¹æºåˆ†æ</h2>

        <h3>1.1 æ ¸å¿ƒçŸ›ç›¾</h3>
        <div class="problem-box">
            <strong>æœ¬è´¨å†²çªï¼šè®¾è®¡çº¦æŸ vs. æ¨¡å‹è‡ªç„¶å‘æŒ¥</strong>
            <ul>
                <li><strong>è®¾è®¡ä¾§ï¼š</strong>æ¡†æ¶éœ€è¦è®¾ç½®max_tokensæ¥æ§åˆ¶æˆæœ¬ã€æ€§èƒ½å’Œå“åº”æ—¶é—´</li>
                <li><strong>æ¨¡å‹ä¾§ï¼š</strong>LLMåœ¨æŸäº›å¤æ‚å¯¹è¯æµç¨‹ä¸­éœ€è¦æ›´å¤štokensæ‰èƒ½å®Œæ•´è¡¨è¾¾</li>
                <li><strong>åæœï¼š</strong>å¼ºè¡Œæˆªæ–­ä¼šä¸¥é‡å½±å“ä¸–ç•Œæ¨¡æ‹Ÿçš„è¿è´¯æ€§å’ŒçœŸå®æ€§</li>
            </ul>
        </div>

        <h3>1.2 Geminiæ¨¡å‹ç‰¹æ®Šæ€§</h3>
        <div class="warning-box">
            <strong>âš ï¸ Geminiæ¨¡å‹çš„ä¸‰ä¸ªå…³é”®ç‰¹æ€§</strong>

            <p><strong>ç‰¹æ€§1ï¼šmax_tokensä¸‹é™è¦æ±‚</strong></p>
            <p class="code-ref">google_aistudio_model.py:211</p>
            <pre><code>max_tokens=256,  # This is wasteful, but Gemini blocks lower values.</code></pre>
            <p>Geminiè¦æ±‚max_tokensè‡³å°‘ä¸º256ï¼Œå¦åˆ™å¯èƒ½è¿”å›ç©ºå“åº”æˆ–é˜»æ­¢è¯·æ±‚ã€‚</p>

            <p><strong>ç‰¹æ€§2ï¼šå®‰å…¨è¿‡æ»¤æœºåˆ¶</strong></p>
            <p class="code-ref">google_aistudio_model.py:75-92</p>
            <pre><code>DEFAULT_SAFETY_SETTINGS = (
    {'category': 'HARM_CATEGORY_HARASSMENT', 'threshold': 'BLOCK_MEDIUM_AND_ABOVE'},
    {'category': 'HARM_CATEGORY_HATE_SPEECH', 'threshold': 'BLOCK_MEDIUM_AND_ABOVE'},
    {'category': 'HARM_CATEGORY_SEXUALLY_EXPLICIT', 'threshold': 'BLOCK_MEDIUM_AND_ABOVE'},
    {'category': 'HARM_CATEGORY_DANGEROUS_CONTENT', 'threshold': 'BLOCK_MEDIUM_AND_ABOVE'},
)</code></pre>
            <p>å®‰å…¨è¿‡æ»¤å¯èƒ½å¯¼è‡´å“åº”è¢«é˜»æ­¢ï¼Œè¿”å›ç©ºç»“æœã€‚</p>

            <p><strong>ç‰¹æ€§3ï¼šç©ºå“åº”è¿”å›æ–¹å¼</strong></p>
            <p>å½“max_tokensè€—å°½æˆ–è§¦å‘å®‰å…¨è¿‡æ»¤æ—¶ï¼Œ<code>sample.candidates</code>å¯èƒ½ä¸ºç©ºæ•°ç»„ï¼Œè®¿é—®<code>candidates[0]</code>ä¼šæŠ›å‡ºIndexErrorã€‚</p>
        </div>

        <h3>1.3 æ¡†æ¶ä¸­çš„max_tokensé…ç½®</h3>
        <table>
            <thead>
                <tr>
                    <th>ç»„ä»¶/æ–¹æ³•</th>
                    <th>é»˜è®¤max_tokens</th>
                    <th>ä»£ç ä½ç½®</th>
                    <th>é£é™©ç­‰çº§</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>DEFAULT_MAX_TOKENS (åŸºç¡€)</td>
                    <td>50</td>
                    <td>interactive_document.py:27-28</td>
                    <td class="severity-high">é«˜</td>
                </tr>
                <tr>
                    <td>question_of_recent_memories</td>
                    <td>1000</td>
                    <td>question_of_recent_memories.py:147</td>
                    <td class="severity-medium">ä¸­</td>
                </tr>
                <tr>
                    <td>all_similar_memories (æ‘˜è¦)</td>
                    <td>750</td>
                    <td>all_similar_memories.py:82</td>
                    <td class="severity-medium">ä¸­</td>
                </tr>
                <tr>
                    <td>concat_act_component (è¡ŒåŠ¨)</td>
                    <td>2200</td>
                    <td>concat_act_component.py:141</td>
                    <td class="severity-low">ä½</td>
                </tr>
                <tr>
                    <td>thought_chains (å„æ­¥éª¤)</td>
                    <td>1200-3500</td>
                    <td>thought_chains.py:å„å¤„</td>
                    <td class="severity-medium">ä¸­</td>
                </tr>
                <tr>
                    <td>open_question_diversified</td>
                    <td>50 Ã— num_samples</td>
                    <td>interactive_document.py:275</td>
                    <td class="severity-high">é«˜</td>
                </tr>
            </tbody>
        </table>

        <h2 id="failure-scenarios">2ï¸âƒ£ å…¸å‹å¤±è´¥åœºæ™¯</h2>

        <h3>2.1 åœºæ™¯1ï¼šopen_question_diversified å´©æºƒ</h3>
        <div class="problem-box">
            <strong>å¤±è´¥è·¯å¾„ï¼š</strong>
            <div class="flow-diagram">
ç”¨æˆ·è°ƒç”¨ open_question_diversified(num_samples=10)
    â†“
è®¡ç®— max_tokens = 50 Ã— 10 = 500 (å°äºGeminiæœ€å°å€¼256)
    â†“
Geminiè¿”å›ç©ºå“åº” â†’ sample.candidates = []
    â†“
google_aistudio_model.py:178 è®¿é—® candidates[0]
    â†“
âŒ IndexError (å› ä¸ºåªæ•è·äº†ValueError)
    â†“
é€€å›åˆ° sample_text è¿”å›ç©ºå­—ç¬¦ä¸² ''
    â†“
interactive_document.py:283 â†’ candidates.splitlines() â†’ []
    â†“
line 294 â†’ random.choice([])
    â†“
ğŸ’¥ <strong>IndexError: Cannot choose from an empty sequence</strong>
            </div>

            <p><strong>ä»£ç ä½ç½®ï¼š</strong></p>
            <p class="code-ref">interactive_document.py:273-294</p>
            <pre><code>candidates = self._model.sample_text(
    prompt=self._model_view.text(),
    max_tokens=max_tokens * num_samples,  # å¯èƒ½å°äº256
    ...
)
candidates = candidates.splitlines()  # ç©ºå­—ç¬¦ä¸² â†’ []
...
response = random.choice(candidates)  # âŒ IndexError!</code></pre>
        </div>

        <h3>2.2 åœºæ™¯2ï¼šthought_chains ç©ºäº‹ä»¶æ³¨å…¥</h3>
        <div class="problem-box">
            <strong>å¤±è´¥è·¯å¾„ï¼š</strong>
            <div class="flow-diagram">
EventResolutionä¸­è°ƒç”¨ maybe_inject_narrative_push
    â†“
ç”Ÿæˆ plausible_events (max_tokens=1500)
    â†“
LLMå› å†…å®¹å¤æ‚è¶…å‡ºé™åˆ¶ï¼Œè¢«æˆªæ–­æˆ–è¿”å›ä¸å®Œæ•´åˆ—è¡¨
    â†“
thought_chains.py:681 â†’ plausible_events.split('\n')
    â†“
å¾—åˆ° ['', '', ''] (ç©ºè¡Œæˆ–ä¸å®Œæ•´)
    â†“
line 683 â†’ random.choice(plausible_events)
    â†“
âš ï¸ é€‰ä¸­ç©ºå­—ç¬¦ä¸²ï¼Œæ³¨å…¥æ— æ•ˆäº‹ä»¶åˆ°ä¸–ç•Œæ¨¡æ‹Ÿ
    â†“
ğŸ­ <strong>ä¸–ç•Œæ¨¡æ‹Ÿè´¨é‡ä¸¥é‡ä¸‹é™</strong>
            </div>

            <p class="code-ref">thought_chains.py:675-683</p>
        </div>

        <h3>2.3 åœºæ™¯3ï¼šå¤šè½®å¯¹è¯ä¸­çš„ç´¯ç§¯æˆªæ–­</h3>
        <div class="warning-box">
            <strong>æ¸è¿›å¼å¤±è´¥ï¼š</strong>
            <ol>
                <li><strong>ç¬¬1è½®ï¼š</strong>LLMè¾“å‡ºè¢«æˆªæ–­ï¼Œä¸¢å¤±å…³é”®ä¸Šä¸‹æ–‡ï¼ˆå¦‚è§’è‰²çŠ¶æ€å˜åŒ–ï¼‰</li>
                <li><strong>ç¬¬2è½®ï¼š</strong>åŸºäºä¸å®Œæ•´ä¿¡æ¯çš„æ¨ç†ï¼Œäº§ç”Ÿé€»è¾‘æ–­è£‚</li>
                <li><strong>ç¬¬3è½®ï¼š</strong>é”™è¯¯ç´¯ç§¯ï¼Œè§’è‰²è¡Œä¸ºå¼€å§‹ä¸ä¸€è‡´</li>
                <li><strong>ç¬¬Nè½®ï¼š</strong>ä¸–ç•Œæ¨¡æ‹Ÿå®Œå…¨å¤±å»è¿è´¯æ€§</li>
            </ol>
            <p><strong>å½±å“èŒƒå›´ï¼š</strong>æ•´ä¸ªæ¸¸æˆå¾ªç¯çš„100-1000æ­¥æ¨¡æ‹Ÿ</p>
        </div>

        <h2 id="code-defects">3ï¸âƒ£ ä»£ç ç¼ºé™·å®šä½</h2>

        <h3>3.1 ç¼ºé™·çŸ©é˜µ</h3>
        <table>
            <thead>
                <tr>
                    <th>æ–‡ä»¶</th>
                    <th>è¡Œå·</th>
                    <th>ç¼ºé™·ç±»å‹</th>
                    <th>ä¸¥é‡ç¨‹åº¦</th>
                    <th>å½±å“</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>google_aistudio_model.py</td>
                    <td>177-183</td>
                    <td>å¼‚å¸¸æ•è·ä¸å®Œæ•´</td>
                    <td class="severity-high">é«˜</td>
                    <td>åªæ•è·ValueErrorï¼Œæœªæ•è·IndexError</td>
                </tr>
                <tr>
                    <td>cloud_vertex_model.py</td>
                    <td>134-140</td>
                    <td>å¼‚å¸¸æ•è·ä¸å®Œæ•´</td>
                    <td class="severity-high">é«˜</td>
                    <td>åŒä¸Š</td>
                </tr>
                <tr>
                    <td>interactive_document.py</td>
                    <td>294</td>
                    <td>ç©ºæ•°ç»„æœªæ£€æŸ¥</td>
                    <td class="severity-high">é«˜</td>
                    <td>random.choice([]) â†’ IndexError</td>
                </tr>
                <tr>
                    <td>interactive_document.py</td>
                    <td>289-292</td>
                    <td>Warningä¸é˜»æ­¢æ‰§è¡Œ</td>
                    <td class="severity-high">é«˜</td>
                    <td>raise Warningä¸ä¼šä¸­æ–­ï¼Œç»§ç»­æ‰§è¡Œå¯¼è‡´é”™è¯¯</td>
                </tr>
                <tr>
                    <td>thought_chains.py</td>
                    <td>681-683</td>
                    <td>ç©ºå­—ç¬¦ä¸²æœªæ£€æŸ¥</td>
                    <td class="severity-medium">ä¸­</td>
                    <td>splitåå¯èƒ½å¾—åˆ°ç©ºå…ƒç´ </td>
                </tr>
                <tr>
                    <td>å¤šå¤„ç»„ä»¶</td>
                    <td>N/A</td>
                    <td>ç¼ºå°‘é‡è¯•æœºåˆ¶</td>
                    <td class="severity-medium">ä¸­</td>
                    <td>ä¸€æ¬¡å¤±è´¥å³æ”¾å¼ƒ</td>
                </tr>
            </tbody>
        </table>

        <h3>3.2 ç¼ºé™·è¯¦ç»†åˆ†æ</h3>

        <h4>ç¼ºé™·1: å¼‚å¸¸æ•è·ä¸å®Œæ•´ (google_aistudio_model.py:177-183)</h4>
        <pre><code>try:
    response = sample.candidates[0].content.parts[0].text
except ValueError as e:  # âŒ åªæ•è·ValueError
    print('An error occurred: ', e)
    print(f'prompt: {prompt}')
    print(f'sample: {sample}')
    response = ''</code></pre>

        <div class="problem-box">
            <strong>é—®é¢˜ï¼š</strong>
            <ul>
                <li>å½“<code>sample.candidates</code>ä¸ºç©ºæ•°ç»„æ—¶ï¼Œ<code>candidates[0]</code>ä¼šæŠ›å‡º<code>IndexError</code></li>
                <li>å½“å‰åªæ•è·<code>ValueError</code>ï¼Œ<code>IndexError</code>ä¼šå¯¼è‡´ç¨‹åºå´©æºƒ</li>
                <li>å³ä½¿æ•è·ï¼Œè¿”å›ç©ºå­—ç¬¦ä¸²<code>''</code>ä¹Ÿæ²¡æœ‰è¢«ä¸Šå±‚ä»£ç é€‚å½“å¤„ç†</li>
            </ul>
        </div>

        <h4>ç¼ºé™·2: Warningä¸é˜»æ­¢æ‰§è¡Œ (interactive_document.py:289-292)</h4>
        <pre><code>if len(candidates) < 2:
    raise Warning(  # âŒ Warningä¸ä¼šä¸­æ–­æ‰§è¡Œ
        f'LLM generated only {len(candidates)} initial answers.'
    )
candidates = [re.sub(r'^\d+\.\s*', '', line) for line in candidates]
response = random.choice(candidates)  # ç»§ç»­æ‰§è¡Œï¼Œå¯¼è‡´IndexError</code></pre>

        <div class="problem-box">
            <strong>é—®é¢˜ï¼š</strong>
            <ul>
                <li><code>raise Warning</code>åœ¨Pythonä¸­ä¸ä¼šä¸­æ–­æ‰§è¡Œï¼ˆä¸<code>raise Exception</code>ä¸åŒï¼‰</li>
                <li>ç¨‹åºç»§ç»­æ‰§è¡Œåˆ°<code>random.choice([])</code>ï¼Œå¯¼è‡´<code>IndexError</code></li>
                <li>åº”è¯¥ä½¿ç”¨<code>raise ValueError</code>æˆ–æä¾›é»˜è®¤å“åº”</li>
            </ul>
        </div>

        <h2 id="solutions">4ï¸âƒ£ å¤šå±‚æ¬¡è§£å†³æ–¹æ¡ˆ</h2>

        <h3>4.1 è§£å†³æ–¹æ¡ˆæ¶æ„</h3>
        <div class="info-box">
            <strong>ä¸‰å±‚é˜²å¾¡ä½“ç³»ï¼š</strong>
            <ol>
                <li><strong>åº•å±‚ï¼ˆModelå±‚ï¼‰ï¼š</strong>å®Œå–„å¼‚å¸¸å¤„ç†ï¼Œç¡®ä¿æ°¸ä¸å´©æºƒ</li>
                <li><strong>ä¸­å±‚ï¼ˆDocumentå±‚ï¼‰ï¼š</strong>æ™ºèƒ½é‡è¯•ä¸é™çº§ç­–ç•¥</li>
                <li><strong>ä¸Šå±‚ï¼ˆComponentå±‚ï¼‰ï¼š</strong>è‡ªé€‚åº”max_tokensä¸è´¨é‡ç›‘æ§</li>
            </ol>
        </div>

        <h3>4.2 Layer 1: Modelå±‚å¢å¼ºå¼‚å¸¸å¤„ç†</h3>
        <div class="solution-box">
            <strong>âœ“ è§£å†³ç›®æ ‡ï¼š</strong>
            <ul class="checklist">
                <li>æ•è·æ‰€æœ‰å¯èƒ½çš„å¼‚å¸¸ç±»å‹ï¼ˆIndexError, ValueError, AttributeErrorç­‰ï¼‰</li>
                <li>æä¾›æœ‰æ„ä¹‰çš„é»˜è®¤å“åº”è€Œéç©ºå­—ç¬¦ä¸²</li>
                <li>è®°å½•è¯¦ç»†çš„å¤±è´¥ä¿¡æ¯ç”¨äºè°ƒè¯•å’Œç›‘æ§</li>
                <li>ä¸ºä¸Šå±‚æä¾›å¤±è´¥ä¿¡å·ï¼ˆå¦‚è¿”å›ç‰¹æ®Šæ ‡è®°æˆ–æŠ›å‡ºè‡ªå®šä¹‰å¼‚å¸¸ï¼‰</li>
            </ul>
        </div>

        <h3>4.3 Layer 2: Documentå±‚æ™ºèƒ½é‡è¯•</h3>
        <div class="solution-box">
            <strong>âœ“ ç­–ç•¥ç»„åˆï¼š</strong>
            <ul class="checklist">
                <li><strong>ç­–ç•¥1ï¼š</strong>ç©ºå“åº”æ£€æµ‹ â†’ è‡ªåŠ¨å¢åŠ max_tokensé‡è¯•ï¼ˆÃ—1.5, Ã—2, Ã—3ï¼‰</li>
                <li><strong>ç­–ç•¥2ï¼š</strong>æ¸©åº¦è°ƒèŠ‚ â†’ é¦–æ¬¡å¤±è´¥åé€æ­¥æé«˜æ¸©åº¦å¢åŠ å¤šæ ·æ€§</li>
                <li><strong>ç­–ç•¥3ï¼š</strong>é™çº§å“åº” â†’ å¤šæ¬¡å¤±è´¥åä½¿ç”¨ç®€åŒ–é—®é¢˜æˆ–é»˜è®¤ç­”æ¡ˆ</li>
                <li><strong>ç­–ç•¥4ï¼š</strong>å€™é€‰æ± å¤‡ä»½ â†’ ä¿å­˜å†å²æœ‰æ•ˆå“åº”ä½œä¸ºåº”æ€¥æ± </li>
            </ul>
        </div>

        <h3>4.4 Layer 3: Componentå±‚è‡ªé€‚åº”ä¼˜åŒ–</h3>
        <div class="solution-box">
            <strong>âœ“ è‡ªé€‚åº”æœºåˆ¶ï¼š</strong>
            <ul class="checklist">
                <li><strong>åŠ¨æ€max_tokensï¼š</strong>æ ¹æ®å†å²æˆªæ–­ç‡è‡ªåŠ¨è°ƒæ•´</li>
                <li><strong>ä¸Šä¸‹æ–‡å‹ç¼©ï¼š</strong>å½“æ¥è¿‘é™åˆ¶æ—¶ï¼Œå‹ç¼©éå…³é”®ä¿¡æ¯</li>
                <li><strong>åˆ†æ®µç”Ÿæˆï¼š</strong>å°†é•¿è¾“å‡ºæ‹†åˆ†ä¸ºå¤šä¸ªå°æ®µç”Ÿæˆååˆå¹¶</li>
                <li><strong>è´¨é‡ç›‘æ§ï¼š</strong>å®æ—¶æ£€æµ‹è¾“å‡ºå®Œæ•´æ€§å’Œè¿è´¯æ€§</li>
            </ul>
        </div>

        <h2 id="implementation">5ï¸âƒ£ ä»£ç å®ç°æ–¹æ¡ˆ</h2>

        <h3>5.1 æ–¹æ¡ˆA: Modelå±‚å¥å£®æ€§å¢å¼º</h3>
        <p><strong>ä¿®æ”¹æ–‡ä»¶ï¼š</strong> <code>concordia/language_model/google_aistudio_model.py</code></p>
        <p><strong>ä¿®æ”¹ä½ç½®ï¼š</strong> line 177-183</p>

        <pre><code># ===== åŸä»£ç  =====
try:
    response = sample.candidates[0].content.parts[0].text
except ValueError as e:
    print('An error occurred: ', e)
    print(f'prompt: {prompt}')
    print(f'sample: {sample}')
    response = ''

# ===== æ”¹è¿›ç‰ˆæœ¬ =====
try:
    # æ£€æŸ¥candidatesæ˜¯å¦ä¸ºç©º
    if not sample.candidates:
        raise IndexError('No candidates returned by the model')

    # å°è¯•è·å–å“åº”æ–‡æœ¬
    response = sample.candidates[0].content.parts[0].text

    # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
    if not response or not response.strip():
        raise ValueError('Empty response from model')

except (ValueError, IndexError, AttributeError) as e:
    # å®Œæ•´çš„å¼‚å¸¸æ•è·
    print(f'âŒ LLM Response Error: {type(e).__name__}: {e}')
    print(f'Prompt (first 200 chars): {prompt[:200]}...')
    print(f'Sample: {sample}')

    # æ£€æŸ¥æ˜¯å¦å› ä¸ºå®‰å…¨è¿‡æ»¤
    if sample.candidates and hasattr(sample.candidates[0], 'safety_ratings'):
        print(f'Safety ratings: {sample.candidates[0].safety_ratings}')

    # æ£€æŸ¥finish_reason
    if sample.candidates and hasattr(sample.candidates[0], 'finish_reason'):
        finish_reason = sample.candidates[0].finish_reason
        print(f'Finish reason: {finish_reason}')

        # æ ¹æ®finish_reasonæä¾›æ›´å¥½çš„é»˜è®¤å“åº”
        if 'SAFETY' in str(finish_reason):
            response = '[RESPONSE_BLOCKED_BY_SAFETY_FILTER]'
        elif 'MAX_TOKENS' in str(finish_reason) or 'LENGTH' in str(finish_reason):
            response = '[RESPONSE_TRUNCATED_BY_MAX_TOKENS]'
        else:
            response = '[RESPONSE_GENERATION_FAILED]'
    else:
        response = '[MODEL_RETURNED_NO_CANDIDATES]'

    # è®°å½•åˆ°æµ‹é‡ç³»ç»Ÿï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if self._measurements is not None:
        self._measurements.publish_datum(
            self._channel,
            {
                'error_type': type(e).__name__,
                'error_message': str(e),
                'has_candidates': bool(sample.candidates),
            }
        )</code></pre>

        <h3>5.2 æ–¹æ¡ˆB: Documentå±‚æ™ºèƒ½é‡è¯•</h3>
        <p><strong>ä¿®æ”¹æ–‡ä»¶ï¼š</strong> <code>concordia/document/interactive_document.py</code></p>
        <p><strong>ä¿®æ”¹æ–¹æ³•ï¼š</strong> <code>open_question_diversified</code></p>

        <pre><code>def open_question_diversified(
    self,
    question: str,
    *,
    forced_response: str | None = None,
    num_samples: int = 10,
    max_tokens: int = DEFAULT_MAX_TOKENS,
    temperature: float = language_model.DEFAULT_TEMPERATURE,
    top_p: float = language_model.DEFAULT_TOP_P,
    top_k: int = language_model.DEFAULT_TOP_K,
    terminators: Collection[str] = (),
    question_label: str = 'Question',
    answer_label: str = 'Answer',
    # æ–°å¢å‚æ•°
    max_retries: int = 3,
    min_valid_candidates: int = 2,
    fallback_response: str | None = None,
) -> str:
    """å¢å¼ºç‰ˆï¼šå¸¦é‡è¯•å’Œé™çº§çš„å¤šæ ·åŒ–é—®ç­”"""

    # ... truncate_stringå®šä¹‰ä¿æŒä¸å˜ ...

    self._question(
        f'Task: generate {num_samples} {answer_label}s to the following'
        f' {question_label}:\nQuestion: {question}\n'
    )

    if forced_response is not None:
        # å¼ºåˆ¶å“åº”è·¯å¾„ä¿æŒä¸å˜
        self._response(f'Final {answer_label}: ')
        self._model_response(f'{forced_response}\n')
        return forced_response

    # ===== æ–°å¢ï¼šæ™ºèƒ½é‡è¯•é€»è¾‘ =====
    candidates = []
    current_max_tokens = max_tokens * num_samples
    current_temperature = temperature

    for attempt in range(max_retries):
        # ç¡®ä¿max_tokensæ»¡è¶³Geminiæœ€å°è¦æ±‚
        current_max_tokens = max(current_max_tokens, 256)

        self._response(f'{answer_label}s (attempt {attempt+1}):\n1. ')

        raw_response = self._model.sample_text(
            prompt=self._model_view.text(),
            max_tokens=current_max_tokens,
            terminators=[],
            temperature=current_temperature,
            top_p=top_p,
            top_k=top_k,
        )

        # æ£€æŸ¥å“åº”æ˜¯å¦æœ‰æ•ˆ
        if not raw_response or raw_response.startswith('[RESPONSE_'):
            self.debug(f'Attempt {attempt+1} failed: {raw_response}')

            # è°ƒæ•´å‚æ•°å‡†å¤‡é‡è¯•
            current_max_tokens = int(current_max_tokens * 1.5)
            current_temperature = min(current_temperature + 0.2, 1.5)
            continue

        self.statement(raw_response)

        # è§£æå€™é€‰ç­”æ¡ˆ
        candidates = [line.strip() for line in raw_response.splitlines() if line.strip()]
        candidates = [re.sub(r'^\d+\.\s*', '', line) for line in candidates]
        candidates = [c for c in candidates if c]  # ç§»é™¤ç©ºå­—ç¬¦ä¸²

        # æ£€æŸ¥æ˜¯å¦è·å¾—è¶³å¤Ÿçš„æœ‰æ•ˆå€™é€‰
        if len(candidates) >= min_valid_candidates:
            self.debug(f'Successfully got {len(candidates)} candidates on attempt {attempt+1}')
            break
        else:
            self.debug(
                f'Attempt {attempt+1}: Only got {len(candidates)} candidates, '
                f'need at least {min_valid_candidates}'
            )

            # è°ƒæ•´å‚æ•°å‡†å¤‡é‡è¯•
            current_max_tokens = int(current_max_tokens * 1.5)
            current_temperature = min(current_temperature + 0.2, 1.5)

    # ===== æ–°å¢ï¼šé™çº§å¤„ç† =====
    if len(candidates) == 0:
        # å®Œå…¨å¤±è´¥ï¼Œä½¿ç”¨é™çº§å“åº”
        if fallback_response:
            response = fallback_response
            self.debug(f'Using fallback response: {response}')
        else:
            # ç®€åŒ–é—®é¢˜ï¼Œåªè¦æ±‚å•ä¸ªç­”æ¡ˆ
            self.debug('Falling back to simple open_question')
            response = self.open_question(
                question=question,
                max_tokens=max_tokens * 3,  # ç»™æ›´å¤štoken
                temperature=current_temperature,
                question_label=question_label,
                answer_label=answer_label,
            )
    elif len(candidates) == 1:
        # åªæœ‰ä¸€ä¸ªå€™é€‰ï¼Œç›´æ¥ä½¿ç”¨
        response = candidates[0]
        self.debug(f'Only one candidate available: {response}')
    else:
        # æ­£å¸¸è·¯å¾„ï¼šä»å€™é€‰ä¸­éšæœºé€‰æ‹©
        response = random.choice(candidates)
        response = truncate_string(response, terminators)

    self._response(f'Final {answer_label}: ')
    self._model_response(f'{response}\n')
    return response</code></pre>

        <h3>5.3 æ–¹æ¡ˆC: è‡ªé€‚åº”Max Tokensç®¡ç†å™¨</h3>
        <p><strong>æ–°å¢æ–‡ä»¶ï¼š</strong> <code>concordia/utils/adaptive_tokens.py</code></p>

        <pre><code>"""è‡ªé€‚åº”Tokenç®¡ç†ï¼šæ ¹æ®è¿è¡Œæ—¶è¡¨ç°åŠ¨æ€è°ƒæ•´max_tokens"""

from collections import deque
from dataclasses import dataclass
from typing import Optional
import time


@dataclass
class TokenUsageStats:
    """Tokenä½¿ç”¨ç»Ÿè®¡"""
    requested: int
    actual: int  # å®é™…ä½¿ç”¨çš„tokenæ•°
    truncated: bool
    latency: float  # å“åº”å»¶è¿Ÿï¼ˆç§’ï¼‰
    timestamp: float


class AdaptiveTokenManager:
    """è‡ªé€‚åº”Tokenç®¡ç†å™¨

    æ ¹æ®å†å²è¡¨ç°è‡ªåŠ¨è°ƒæ•´max_tokensï¼Œå¹³è¡¡è´¨é‡ã€æˆæœ¬å’Œå»¶è¿Ÿã€‚
    """

    def __init__(
        self,
        base_max_tokens: int = 1000,
        min_max_tokens: int = 256,  # Geminiæœ€å°å€¼
        max_max_tokens: int = 8192,
        history_window: int = 100,
        truncation_threshold: float = 0.3,  # æˆªæ–­ç‡é˜ˆå€¼
        adjustment_factor: float = 1.5,
    ):
        self.base_max_tokens = base_max_tokens
        self.min_max_tokens = min_max_tokens
        self.max_max_tokens = max_max_tokens
        self.history_window = history_window
        self.truncation_threshold = truncation_threshold
        self.adjustment_factor = adjustment_factor

        self._history: deque[TokenUsageStats] = deque(maxlen=history_window)
        self._current_max_tokens = base_max_tokens

    def record_usage(
        self,
        requested: int,
        actual: int,
        truncated: bool,
        latency: float,
    ) -> None:
        """è®°å½•ä¸€æ¬¡tokenä½¿ç”¨"""
        self._history.append(TokenUsageStats(
            requested=requested,
            actual=actual,
            truncated=truncated,
            latency=latency,
            timestamp=time.time(),
        ))

        # è§¦å‘è‡ªé€‚åº”è°ƒæ•´
        self._adjust_if_needed()

    def get_max_tokens(self, context: Optional[str] = None) -> int:
        """è·å–å½“å‰æ¨èçš„max_tokens

        Args:
            context: å¯é€‰çš„ä¸Šä¸‹æ–‡æ ‡è¯†ï¼Œç”¨äºä¸åŒåœºæ™¯ä½¿ç”¨ä¸åŒç­–ç•¥

        Returns:
            æ¨èçš„max_tokenså€¼
        """
        # ç¡®ä¿ä¸ä½äºæœ€å°å€¼ï¼ˆGeminiè¦æ±‚ï¼‰
        return max(self._current_max_tokens, self.min_max_tokens)

    def _adjust_if_needed(self) -> None:
        """æ ¹æ®å†å²è¡¨ç°è°ƒæ•´max_tokens"""
        if len(self._history) < 10:  # æ•°æ®ä¸è¶³
            return

        # è®¡ç®—æœ€è¿‘çš„æˆªæ–­ç‡
        recent_truncations = sum(1 for stat in self._history if stat.truncated)
        truncation_rate = recent_truncations / len(self._history)

        # è®¡ç®—å¹³å‡ä½¿ç”¨ç‡
        avg_usage_rate = sum(
            stat.actual / stat.requested
            for stat in self._history
        ) / len(self._history)

        # å†³ç­–é€»è¾‘
        if truncation_rate > self.truncation_threshold:
            # æˆªæ–­å¤ªå¤šï¼Œå¢åŠ max_tokens
            new_value = int(self._current_max_tokens * self.adjustment_factor)
            self._current_max_tokens = min(new_value, self.max_max_tokens)
            print(f'ğŸ“ˆ Increasing max_tokens to {self._current_max_tokens} '
                  f'(truncation rate: {truncation_rate:.1%})')

        elif avg_usage_rate < 0.6 and truncation_rate < 0.05:
            # ä½¿ç”¨ç‡ä½ä¸”å¾ˆå°‘æˆªæ–­ï¼Œå‡å°‘max_tokensèŠ‚çœæˆæœ¬
            new_value = int(self._current_max_tokens / self.adjustment_factor)
            self._current_max_tokens = max(new_value, self.min_max_tokens)
            print(f'ğŸ“‰ Decreasing max_tokens to {self._current_max_tokens} '
                  f'(usage rate: {avg_usage_rate:.1%})')

    def get_stats_summary(self) -> dict:
        """è·å–ç»Ÿè®¡æ‘˜è¦"""
        if not self._history:
            return {}

        truncation_rate = sum(1 for s in self._history if s.truncated) / len(self._history)
        avg_latency = sum(s.latency for s in self._history) / len(self._history)
        avg_usage_rate = sum(s.actual / s.requested for s in self._history) / len(self._history)

        return {
            'current_max_tokens': self._current_max_tokens,
            'truncation_rate': truncation_rate,
            'avg_usage_rate': avg_usage_rate,
            'avg_latency': avg_latency,
            'total_calls': len(self._history),
        }


# ===== ä½¿ç”¨ç¤ºä¾‹ =====

# åœ¨InteractiveDocumentä¸­é›†æˆ
class AdaptiveInteractiveDocument(InteractiveDocument):
    """å¸¦è‡ªé€‚åº”tokenç®¡ç†çš„æ–‡æ¡£"""

    def __init__(self, model, contents=(), rng=None):
        super().__init__(model, contents, rng)
        self._token_manager = AdaptiveTokenManager()

    def open_question(self, question: str, **kwargs):
        """å¢å¼ºç‰ˆopen_questionï¼Œè‡ªåŠ¨ä¼˜åŒ–max_tokens"""
        # è·å–æ¨èçš„max_tokens
        if 'max_tokens' not in kwargs:
            kwargs['max_tokens'] = self._token_manager.get_max_tokens()

        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()

        # è°ƒç”¨åŸæ–¹æ³•
        response = super().open_question(question, **kwargs)

        # è®°å½•ä½¿ç”¨æƒ…å†µ
        latency = time.time() - start_time
        truncated = response.endswith('...')  # ç®€åŒ–æ£€æµ‹
        actual_tokens = len(response) // 4  # ç²—ç•¥ä¼°è®¡

        self._token_manager.record_usage(
            requested=kwargs['max_tokens'],
            actual=actual_tokens,
            truncated=truncated,
            latency=latency,
        )

        return response

    def get_token_stats(self):
        """è·å–tokenä½¿ç”¨ç»Ÿè®¡"""
        return self._token_manager.get_stats_summary()</code></pre>

        <h3>5.4 æ–¹æ¡ˆD: å…¨å±€é…ç½®è¦†ç›–ï¼ˆå¿«é€Ÿä¿®å¤ï¼‰</h3>
        <p><strong>é€‚ç”¨åœºæ™¯ï¼š</strong>ç´§æ€¥ä¿®å¤ï¼Œä¸ä¿®æ”¹æºç </p>

        <pre><code># ===== åœ¨å¯åŠ¨è„šæœ¬ä¸­æ·»åŠ  =====

import concordia.document.interactive_document as interactive_doc
import concordia.language_model.language_model as lm

# æ–¹æ³•1: æé«˜å…¨å±€é»˜è®¤å€¼
interactive_doc.DEFAULT_MAX_CHARACTERS = 2000  # åŸå€¼200
interactive_doc.DEFAULT_MAX_TOKENS = 500        # åŸå€¼50
lm.DEFAULT_MAX_TOKENS = 1000                    # åŸå€¼5000ï¼ˆå®é™…å¾ˆå°‘ç”¨ï¼‰

# æ–¹æ³•2: Monkey patchä¿®å¤critical bugs
original_open_question_diversified = interactive_doc.InteractiveDocument.open_question_diversified

def safe_open_question_diversified(self, question, **kwargs):
    """å®‰å…¨åŒ…è£…ç‰ˆæœ¬"""
    try:
        return original_open_question_diversified(self, question, **kwargs)
    except IndexError as e:
        # æ•è·random.choice([])é”™è¯¯
        print(f'âš ï¸ Caught IndexError in open_question_diversified: {e}')
        print(f'Falling back to simple open_question')
        # é™çº§åˆ°ç®€å•ç‰ˆæœ¬
        return self.open_question(
            question=question,
            max_tokens=kwargs.get('max_tokens', 500) * 3,
            temperature=kwargs.get('temperature', 1.0),
        )

# æ›¿æ¢æ–¹æ³•
interactive_doc.InteractiveDocument.open_question_diversified = safe_open_question_diversified

print('âœ… Applied emergency patches for max_tokens issues')</code></pre>

        <h2 id="best-practices">6ï¸âƒ£ æœ€ä½³å®è·µä¸å»ºè®®</h2>

        <h3>6.1 ç«‹å³è¡ŒåŠ¨æ¸…å•</h3>
        <div class="solution-box">
            <strong>ğŸš€ ç«‹å³å®æ–½ï¼ˆç´§æ€¥ï¼‰</strong>
            <ol>
                <li><strong>ä¿®å¤IndexErrorï¼š</strong>åœ¨<code>google_aistudio_model.py</code>å’Œ<code>cloud_vertex_model.py</code>ä¸­å¢åŠ å®Œæ•´å¼‚å¸¸æ•è·</li>
                <li><strong>ä¿®å¤Warningï¼š</strong>å°†<code>interactive_document.py:290</code>çš„<code>raise Warning</code>æ”¹ä¸º<code>raise ValueError</code>æˆ–æä¾›é™çº§å“åº”</li>
                <li><strong>æ·»åŠ ç©ºæ£€æŸ¥ï¼š</strong>åœ¨æ‰€æœ‰<code>random.choice()</code>è°ƒç”¨å‰æ£€æŸ¥æ•°ç»„éç©º</li>
                <li><strong>æé«˜æœ€å°å€¼ï¼š</strong>ç¡®ä¿æ‰€æœ‰max_tokensé…ç½®ä¸ä½äº256ï¼ˆGeminiè¦æ±‚ï¼‰</li>
            </ol>
        </div>

        <h3>6.2 çŸ­æœŸä¼˜åŒ–ï¼ˆ1-2å‘¨ï¼‰</h3>
        <div class="info-box">
            <strong>ğŸ“Š æ•°æ®é©±åŠ¨ä¼˜åŒ–</strong>
            <ol>
                <li><strong>æ·»åŠ ç›‘æ§ï¼š</strong>è®°å½•æ‰€æœ‰LLMè°ƒç”¨çš„tokenä½¿ç”¨ã€æˆªæ–­æƒ…å†µã€å¤±è´¥åŸå› </li>
                <li><strong>åˆ†æçƒ­ç‚¹ï¼š</strong>æ‰¾å‡ºæœ€å®¹æ˜“æˆªæ–­çš„ç»„ä»¶å’Œåœºæ™¯</li>
                <li><strong>è°ƒæ•´é…ç½®ï¼š</strong>åŸºäºæ•°æ®è°ƒé«˜é«˜æˆªæ–­ç‡ç»„ä»¶çš„max_tokens</li>
                <li><strong>å®æ–½é‡è¯•ï¼š</strong>ä¸ºå…³é”®ç»„ä»¶æ·»åŠ æ™ºèƒ½é‡è¯•é€»è¾‘ï¼ˆå‚è€ƒæ–¹æ¡ˆBï¼‰</li>
            </ol>
        </div>

        <h3>6.3 é•¿æœŸæ¶æ„ï¼ˆ1-3ä¸ªæœˆï¼‰</h3>
        <div class="info-box">
            <strong>ğŸ—ï¸ æ¶æ„å‡çº§</strong>
            <ol>
                <li><strong>è‡ªé€‚åº”ç³»ç»Ÿï¼š</strong>å®æ–½AdaptiveTokenManagerï¼ˆæ–¹æ¡ˆCï¼‰</li>
                <li><strong>ä¸Šä¸‹æ–‡å‹ç¼©ï¼š</strong>å½“æ¥è¿‘é™åˆ¶æ—¶æ™ºèƒ½å‹ç¼©éå…³é”®ä¿¡æ¯</li>
                <li><strong>åˆ†æ®µç”Ÿæˆï¼š</strong>æ”¯æŒé•¿è¾“å‡ºåˆ†å¤šæ¬¡ç”Ÿæˆååˆå¹¶</li>
                <li><strong>è´¨é‡ç›‘æ§ï¼š</strong>å®æ—¶æ£€æµ‹è¾“å‡ºå®Œæ•´æ€§ï¼Œè§¦å‘è‡ªåŠ¨é‡è¯•æˆ–é™çº§</li>
                <li><strong>æ¨¡å‹åˆ‡æ¢ï¼š</strong>å…è®¸åœ¨è¿è¡Œæ—¶åˆ‡æ¢ä¸åŒæ¨¡å‹ï¼ˆå¦‚Geminié‡åˆ°é—®é¢˜æ—¶åˆ‡æ¢åˆ°GPT-4ï¼‰</li>
            </ol>
        </div>

        <h3>6.4 é…ç½®æ¨èè¡¨</h3>
        <table>
            <thead>
                <tr>
                    <th>ç»„ä»¶/åœºæ™¯</th>
                    <th>å½“å‰å€¼</th>
                    <th>æ¨èå€¼ï¼ˆä¿å®ˆï¼‰</th>
                    <th>æ¨èå€¼ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰</th>
                    <th>ç†ç”±</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>DEFAULT_MAX_TOKENS</td>
                    <td>50</td>
                    <td>256</td>
                    <td>512</td>
                    <td>Geminiæœ€å°è¦æ±‚ + ç¼“å†²</td>
                </tr>
                <tr>
                    <td>question_of_recent_memories</td>
                    <td>1000</td>
                    <td>1500</td>
                    <td>2500</td>
                    <td>åæ€éœ€è¦å……åˆ†è¡¨è¾¾</td>
                </tr>
                <tr>
                    <td>all_similar_memories (æ‘˜è¦)</td>
                    <td>750</td>
                    <td>1000</td>
                    <td>1500</td>
                    <td>æ‘˜è¦è´¨é‡å½±å“æ£€ç´¢æ•ˆæœ</td>
                </tr>
                <tr>
                    <td>concat_act_component</td>
                    <td>2200</td>
                    <td>2200</td>
                    <td>3000</td>
                    <td>å·²ç»è¾ƒé«˜ï¼Œä¿å®ˆä¸å˜</td>
                </tr>
                <tr>
                    <td>open_question_diversified</td>
                    <td>50 Ã— N</td>
                    <td>max(256, 100 Ã— N)</td>
                    <td>max(512, 150 Ã— N)</td>
                    <td>ç¡®ä¿æ»¡è¶³Geminiè¦æ±‚</td>
                </tr>
                <tr>
                    <td>thought_chains (å„æ­¥éª¤)</td>
                    <td>1200-3500</td>
                    <td>1500-4000</td>
                    <td>2000-5000</td>
                    <td>äº‹ä»¶æ¨ç†éœ€è¦å……åˆ†æ€è€ƒ</td>
                </tr>
            </tbody>
        </table>

        <h3>6.5 ä»£ç å®¡æŸ¥æ£€æŸ¥æ¸…å•</h3>
        <ul class="checklist">
            <li><strong>å¼‚å¸¸å¤„ç†ï¼š</strong>æ˜¯å¦æ•è·äº†æ‰€æœ‰å¯èƒ½çš„å¼‚å¸¸ç±»å‹ï¼ˆIndexError, ValueError, AttributeErrorç­‰ï¼‰</li>
            <li><strong>ç©ºå€¼æ£€æŸ¥ï¼š</strong>è°ƒç”¨<code>random.choice()</code>, <code>[0]</code>, <code>.split()</code>å‰æ˜¯å¦æ£€æŸ¥éç©º</li>
            <li><strong>Geminié™åˆ¶ï¼š</strong>max_tokensæ˜¯å¦ä¸ä½äº256</li>
            <li><strong>é™çº§è·¯å¾„ï¼š</strong>å¤±è´¥æ—¶æ˜¯å¦æœ‰å¤‡ç”¨æ–¹æ¡ˆï¼ˆé‡è¯•ã€ç®€åŒ–ã€é»˜è®¤å€¼ï¼‰</li>
            <li><strong>æ—¥å¿—è®°å½•ï¼š</strong>æ˜¯å¦è®°å½•äº†è¶³å¤Ÿçš„ä¿¡æ¯ç”¨äºè°ƒè¯•ï¼ˆpromptç‰‡æ®µã€é”™è¯¯ç±»å‹ã€finish_reasonç­‰ï¼‰</li>
            <li><strong>ç›‘æ§åŸ‹ç‚¹ï¼š</strong>æ˜¯å¦å°†å…³é”®æŒ‡æ ‡å‘é€åˆ°measurementsç³»ç»Ÿ</li>
        </ul>

        <h3>6.6 è¿è¡Œæ—¶ç›‘æ§æŒ‡æ ‡</h3>
        <table>
            <thead>
                <tr>
                    <th>æŒ‡æ ‡</th>
                    <th>æ­£å¸¸èŒƒå›´</th>
                    <th>è­¦å‘Šé˜ˆå€¼</th>
                    <th>åº”å¯¹æªæ–½</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>æˆªæ–­ç‡</td>
                    <td>&lt; 5%</td>
                    <td>&gt; 15%</td>
                    <td>å¢åŠ max_tokensæˆ–å®æ–½å‹ç¼©</td>
                </tr>
                <tr>
                    <td>ç©ºå“åº”ç‡</td>
                    <td>&lt; 1%</td>
                    <td>&gt; 5%</td>
                    <td>æ£€æŸ¥æ¨¡å‹é…ç½®ï¼Œæ·»åŠ é‡è¯•</td>
                </tr>
                <tr>
                    <td>å¼‚å¸¸ç‡</td>
                    <td>&lt; 0.1%</td>
                    <td>&gt; 1%</td>
                    <td>ç«‹å³ä¿®å¤ä»£ç ç¼ºé™·</td>
                </tr>
                <tr>
                    <td>å¹³å‡tokenä½¿ç”¨ç‡</td>
                    <td>60-90%</td>
                    <td>&lt; 40% æˆ– &gt; 95%</td>
                    <td>è°ƒæ•´max_tokensé…ç½®</td>
                </tr>
                <tr>
                    <td>é‡è¯•æˆåŠŸç‡</td>
                    <td>&gt; 80%</td>
                    <td>&lt; 50%</td>
                    <td>ä¼˜åŒ–é‡è¯•ç­–ç•¥æˆ–é—®é¢˜è®¾è®¡</td>
                </tr>
            </tbody>
        </table>

        <h3>6.7 ä¸åŒæ¨¡å‹çš„é€‚é…å»ºè®®</h3>
        <div class="warning-box">
            <strong>âš™ï¸ æ¨¡å‹ç‰¹æ€§å·®å¼‚</strong>
            <table>
                <thead>
                    <tr>
                        <th>æ¨¡å‹</th>
                        <th>æœ€å°max_tokens</th>
                        <th>æœ€å¤§max_tokens</th>
                        <th>ç‰¹æ®Šæ³¨æ„äº‹é¡¹</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Gemini 1.5 Pro</td>
                        <td>256</td>
                        <td>8192</td>
                        <td>ä¸¥æ ¼çš„å®‰å…¨è¿‡æ»¤ï¼Œå®¹æ˜“è¿”å›ç©º</td>
                    </tr>
                    <tr>
                        <td>GPT-4</td>
                        <td>1</td>
                        <td>4096</td>
                        <td>æˆæœ¬é«˜ï¼Œä½†ç¨³å®šæ€§å¥½</td>
                    </tr>
                    <tr>
                        <td>GPT-3.5 Turbo</td>
                        <td>1</td>
                        <td>4096</td>
                        <td>æ€§ä»·æ¯”é«˜ï¼Œè´¨é‡ç•¥ä½</td>
                    </tr>
                    <tr>
                        <td>Claude 2</td>
                        <td>1</td>
                        <td>100000</td>
                        <td>è¶…é•¿ä¸Šä¸‹æ–‡ï¼Œé€‚åˆå¤æ‚åœºæ™¯</td>
                    </tr>
                </tbody>
            </table>

            <p><strong>å»ºè®®ï¼š</strong>å®æ–½æ¨¡å‹æŠ½è±¡å±‚ï¼Œæ ¹æ®åœºæ™¯è‡ªåŠ¨é€‰æ‹©æœ€åˆé€‚çš„æ¨¡å‹ã€‚ä¾‹å¦‚ï¼š</p>
            <ul>
                <li>ç®€å•é—®ç­” â†’ GPT-3.5 Turboï¼ˆæˆæœ¬ä¼˜åŒ–ï¼‰</li>
                <li>å¤æ‚æ¨ç† â†’ GPT-4 æˆ– Claude 2ï¼ˆè´¨é‡ä¼˜å…ˆï¼‰</li>
                <li>é•¿ä¸Šä¸‹æ–‡ â†’ Claude 2ï¼ˆæŠ€æœ¯ä¼˜åŠ¿ï¼‰</li>
            </ul>
        </div>

        <h3>6.8 æ€»ç»“ï¼šDO vs. DON'T</h3>
        <table>
            <thead>
                <tr>
                    <th style="width: 50%;">âœ… DO (æ¨èåšæ³•)</th>
                    <th style="width: 50%;">âŒ DON'T (é¿å…åšæ³•)</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>æ•è·æ‰€æœ‰å¯èƒ½çš„å¼‚å¸¸ç±»å‹</td>
                    <td>åªæ•è·å•ä¸€å¼‚å¸¸ç±»å‹ï¼ˆå¦‚åªæ•è·ValueErrorï¼‰</td>
                </tr>
                <tr>
                    <td>åœ¨è®¿é—®æ•°ç»„å‰æ£€æŸ¥éç©º</td>
                    <td>å‡è®¾æ•°ç»„æ€»æ˜¯æœ‰å…ƒç´ </td>
                </tr>
                <tr>
                    <td>æä¾›æœ‰æ„ä¹‰çš„é™çº§å“åº”</td>
                    <td>è¿”å›ç©ºå­—ç¬¦ä¸²åä¸å¤„ç†</td>
                </tr>
                <tr>
                    <td>è®°å½•è¯¦ç»†çš„å¤±è´¥ä¿¡æ¯</td>
                    <td>é™é»˜å¤±è´¥ï¼Œä¸è®°å½•æ—¥å¿—</td>
                </tr>
                <tr>
                    <td>å®æ–½æ™ºèƒ½é‡è¯•ï¼ˆå¢åŠ max_tokensã€è°ƒæ•´æ¸©åº¦ï¼‰</td>
                    <td>ä¸€æ¬¡å¤±è´¥å°±æ”¾å¼ƒ</td>
                </tr>
                <tr>
                    <td>æ ¹æ®å†å²è¡¨ç°è‡ªé€‚åº”è°ƒæ•´</td>
                    <td>ä½¿ç”¨å›ºå®šçš„max_tokensé…ç½®</td>
                </tr>
                <tr>
                    <td>ç›‘æ§å…³é”®æŒ‡æ ‡ï¼ˆæˆªæ–­ç‡ã€ç©ºå“åº”ç‡ç­‰ï¼‰</td>
                    <td>ä¸æ”¶é›†è¿è¡Œæ—¶æ•°æ®</td>
                </tr>
                <tr>
                    <td>è€ƒè™‘æ¨¡å‹ç‰¹æ€§å·®å¼‚ï¼ˆå¦‚Geminiæœ€å°256ï¼‰</td>
                    <td>å‡è®¾æ‰€æœ‰æ¨¡å‹è¡Œä¸ºä¸€è‡´</td>
                </tr>
                <tr>
                    <td>åˆ†å±‚é˜²å¾¡ï¼ˆModelå±‚ + Documentå±‚ + Componentå±‚ï¼‰</td>
                    <td>åªåœ¨ä¸€ä¸ªå±‚é¢å¤„ç†</td>
                </tr>
                <tr>
                    <td>å¹³è¡¡è´¨é‡ã€æˆæœ¬å’Œå»¶è¿Ÿ</td>
                    <td>åªè€ƒè™‘æˆæœ¬æˆ–åªè€ƒè™‘è´¨é‡</td>
                </tr>
            </tbody>
        </table>

        <h2>ğŸ“š é™„å½•ï¼šé—®é¢˜è¯Šæ–­æµç¨‹å›¾</h2>
        <div class="flow-diagram">
é‡åˆ°LLMç›¸å…³å¼‚å¸¸
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ­¥éª¤1: è¯†åˆ«å¼‚å¸¸ç±»å‹                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ IndexError: è®¿é—®ç©ºæ•°ç»„/åˆ—è¡¨                         â”‚
â”‚ â€¢ ValueError: æ— æ•ˆå€¼æˆ–æ ¼å¼é”™è¯¯                        â”‚
â”‚ â€¢ AttributeError: å±æ€§ä¸å­˜åœ¨                         â”‚
â”‚ â€¢ InvalidResponseError: è‡ªå®šä¹‰å¼‚å¸¸                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ­¥éª¤2: å®šä½å¤±è´¥å±‚çº§                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Modelå±‚? â†’ æ£€æŸ¥ google_aistudio_model.py:177      â”‚
â”‚ â€¢ Documentå±‚? â†’ æ£€æŸ¥ interactive_document.py:294    â”‚
â”‚ â€¢ Componentå±‚? â†’ æ£€æŸ¥å…·ä½“ç»„ä»¶çš„å®ç°                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ­¥éª¤3: æ£€æŸ¥root cause                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ æ˜¯å¦ max_tokens < 256? (Geminié™åˆ¶)                â”‚
â”‚ â€¢ æ˜¯å¦è§¦å‘å®‰å…¨è¿‡æ»¤? (æ£€æŸ¥safety_ratings)              â”‚
â”‚ â€¢ æ˜¯å¦ finish_reason æ˜¾ç¤º MAX_TOKENS?                â”‚
â”‚ â€¢ æ˜¯å¦æ²¡æœ‰æ•è·æ­£ç¡®çš„å¼‚å¸¸ç±»å‹?                         â”‚
â”‚ â€¢ æ˜¯å¦æ²¡æœ‰æ£€æŸ¥ç©ºå€¼?                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ­¥éª¤4: åº”ç”¨å¯¹åº”è§£å†³æ–¹æ¡ˆ                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ max_tokensä¸è¶³ â†’ å¢åŠ åˆ°è‡³å°‘256ï¼Œæˆ–å®æ–½è‡ªé€‚åº”ç®¡ç†    â”‚
â”‚ â€¢ å®‰å…¨è¿‡æ»¤ â†’ è°ƒæ•´safety_settingsæˆ–æ”¹å†™prompt         â”‚
â”‚ â€¢ å¼‚å¸¸æ•è·ä¸å®Œæ•´ â†’ å‚è€ƒæ–¹æ¡ˆAä¿®å¤                      â”‚
â”‚ â€¢ ç©ºå€¼æœªæ£€æŸ¥ â†’ æ·»åŠ ifæ£€æŸ¥æˆ–å‚è€ƒæ–¹æ¡ˆBå®æ–½é‡è¯•          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æ­¥éª¤5: éªŒè¯ä¿®å¤æ•ˆæœ                                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ è¿è¡Œç›¸åŒåœºæ™¯æµ‹è¯•                                    â”‚
â”‚ â€¢ ç›‘æ§å…³é”®æŒ‡æ ‡ï¼ˆæˆªæ–­ç‡ã€ç©ºå“åº”ç‡ã€å¼‚å¸¸ç‡ï¼‰             â”‚
â”‚ â€¢ é•¿æ—¶é—´è¿è¡Œç¨³å®šæ€§æµ‹è¯•ï¼ˆ100+ stepsï¼‰                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        </div>

        <hr style="margin: 40px 0;">

        <p style="text-align: center; color: #666; font-size: 0.9em;">
            <strong>æ–‡æ¡£ç»´æŠ¤ï¼š</strong>åŸºäºConcordiaæºç åˆ†æ |
            <strong>æœ€åæ›´æ–°ï¼š</strong>2025-01-10 |
            <strong>ç‰ˆæœ¬ï¼š</strong>1.0
        </p>
    </div>
</body>
</html>